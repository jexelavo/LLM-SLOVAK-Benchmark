{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slovak Parliamentary Narrative Analysis\n",
    "\n",
    "This notebook analyzes the effectiveness of embedding models and LLMs on Slovak parliamentary transcripts (2010–2023). The goal is to expand a small set of seed statements into a comprehensive set of statements covering the semantic space of the corpus, with minimal overlap.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. Load and preprocess the corpus.\n",
    "2. Compute embeddings and similarity scores.\n",
    "3. Sample and extract topic-relevant statements.\n",
    "4. Categorize statements using LLMs.\n",
    "5. Iterate until semantic coverage is achieved.\n",
    "\n",
    "*For details on the corpus structure and evaluation, see the README.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Dict, Literal\n",
    "from operator import add\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import TypedDict\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from docx import Document\n",
    "import random\n",
    "import re, unicodedata, json\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(r\"..\\keys.env\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai(role_message,user_message, model=model,reasoning_effort=\"minimal\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": role_message},\n",
    "        {\"role\": \"user\", \"content\": user_message}],\n",
    "        reasoning_effort=reasoning_effort,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r\"..\\data\\df_to_app_with_openAI_S_L_voyage_gdoogle_mistral_embeddings_obdobie_8_with_narratives.parquet\", engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is valid for openai embeddings based on the distribution of similarity scores from cosine_treshold_final.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Edge_low, Edge_high = 0.45, 0.60\n",
    "similarity_threshold = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(input_text):\n",
    "    response = client.embeddings.create(\n",
    "        input=[input_text],  # Ensure input is a list\n",
    "        model=\"text-embedding-3-large\"\n",
    "    )\n",
    "    embedding = np.array(response.data[0].embedding)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def get_combined_narrative_embedding(narrative_list):\n",
    "    narrative_list = [str(item) for item in narrative_list]\n",
    "    combined_narrative = \" \".join(narrative_list)\n",
    "    combined_embedding = get_embeddings(combined_narrative)\n",
    "    return combined_embedding\n",
    "\n",
    "\n",
    "def calculate_similarity(df, embedding_col, ref_embedding, similarity_col):\n",
    "    \"\"\"Calculate cosine similarity between each row embedding and ref_embedding.\n",
    "\n",
    "    Assumes embeddings already have identical dimensionality. If a row embedding\n",
    "    mismatches in length, it is skipped (NaN) instead of alignment.\n",
    "    \"\"\"\n",
    "    ref = np.array(ref_embedding).flatten()\n",
    "    ref_len = ref.shape[0]\n",
    "    sims = []\n",
    "    for emb in df[embedding_col]:\n",
    "        try:\n",
    "            v = np.array(emb).flatten()\n",
    "            if v.shape[0] != ref_len:\n",
    "                sims.append(np.nan)\n",
    "                continue\n",
    "            sims.append(float(cosine_similarity(v.reshape(1, -1), ref.reshape(1, -1))[0][0]))\n",
    "        except Exception:\n",
    "            sims.append(np.nan)\n",
    "    df[similarity_col] = sims\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_slug(name: str) -> str:\n",
    "    name = ''.join(ch for ch in unicodedata.normalize('NFD', name) if unicodedata.category(ch) != 'Mn')\n",
    "    name = re.sub(r'[^a-z0-9\\s-]', '', name.lower()).strip()\n",
    "    name = re.sub(r'\\s+', '-', name)\n",
    "    return name[:80] or 'uncategorized'\n",
    "\n",
    "def parse_extracted_block(block: str) -> list[str]:\n",
    "    if not block:\n",
    "        return []\n",
    "    lines=[]\n",
    "    for raw in block.split('\\n'):\n",
    "        ln = raw.strip()\n",
    "        if not ln:\n",
    "            continue\n",
    "        # remove leading bullets / dashes / numbering\n",
    "        ln = re.sub(r'^[-•\\d\\.\\)\\(]+\\s*', '', ln).strip()\n",
    "        if len(ln) < 5:\n",
    "            continue\n",
    "        lines.append(ln)\n",
    "    dedup=[]\n",
    "    seen=set()\n",
    "    for l in lines:\n",
    "        if l not in seen:\n",
    "            dedup.append(l); seen.add(l)\n",
    "    return dedup\n",
    "\n",
    "def build_category_embeddings(categories_structured: list[dict]):\n",
    "    cat_embs = {}\n",
    "    for cat in categories_structured or []:\n",
    "        slug = _normalize_slug(cat.get('name',''))\n",
    "        items = cat.get('items', [])\n",
    "        if not items:\n",
    "            continue\n",
    "        cat_embs[slug] = get_combined_narrative_embedding(items)\n",
    "    return cat_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_vaccine_queries = [\n",
    "    \"Očkovanie nesmie byt v ziadnom pripade povinne.\",\n",
    "    \"Nikto nevie ake bude mat ockovanie nasledky do buducnosti. Vakcina nebola vobec riadne odskusana. Je to neodskusana latka\",\n",
    "    \"Očkovanie je hlavne velky biznis. My sa tu ockujeme neznamou latkou a farmaceuticke firmy budu mat obrovske zisky\",\n",
    "    \"To ockovanie vobec nefunguje. Ak sa zaockujete stale mozete dostat Covid\"\n",
    "]\n",
    "\n",
    "pro_russian_queries = [\n",
    "    \"Nerobme z Ruska nášho nepriatela Rusko nie je náš nepriateľ.\",\n",
    "    \"Rusko vyprokovalo rozširovanie do NATO. Rusko nikdy nedovolí,  aby Ukrajina bola v NATO\",\n",
    "    \"Dodavanim zbrani ten konflit len predlžujme.Ak budeme posielat zbrane tak ten konflikt predlžime a viac ľudí bude umierať\",\n",
    "    \"Všetci len strašite Ruskom, že Rusko je zle. Ale čo Irak a Juhoslavia čo bombardovali Američania a USA. Prečo ste boli vtedy ticho? \",\n",
    "    \"Sankciami poškudzujeme len seba. Rusko sankciami vobec netrpi. Bez ruskeho plynu to pomrzneme \"\n",
    "]\n",
    "\n",
    "anti_gender_narrative = [\n",
    "    \"Gender  a rodová ideologia nás ohrozuje\",\n",
    "    \"My tu nechceme mať 72 pohlaví\",\n",
    "    \"Pohlavie je sociálny konštrukt podľa gender a rodovej ideologie \",\n",
    "    \"Rodovú a gender ideológia  tvrdí, že nezáleží na vrodenom biologickom pohlaví.\",\n",
    "    \"Rodová a gender ideológia  tvrdí, každý má mať možnosť vybrať si, ci je mužom alebo ženou, alebo niečo medzi tým\",\n",
    "    \"Presadzovať rodovú a gender spolu s LGBTI na školach by malo byt zakazané\"\n",
    "]\n",
    "ekonomicky_liberal = [\"Dane by mali byť, čo najmenšie.\", \"Podnikatelia zamestnávajú ľudí a tvoria hodnoty\",\n",
    "                      \"Potrebujeme, čo najštihlejší štát, či menej úradnikov a úradov tým lepšie\",\n",
    "                      \"Mame pomaly najvyššie odvody v Europskej Unie\", \"Musíme podporovať domácich malých a stredných podnikateľov\", \"Podnikateľov zatažujeme stále väčším počtom reguláccií\", \"Deficit verejných financií musí byť čo najnižší\", \"Nezadlžujte viac už Slovensko\", \"Daňovo odvodové zaťaženi je na Slovensku na neznesitelné\"\n",
    "] \n",
    "anti_smer = [\"SMER je ovladany oligarchami\", \"SMER je mafia, ktorá okrada Slovensko\", \"Financne skupiny okradaju Slovennsko\", \"Mafia je na policii, Mafia je na sudoch. SMER nechal vyrast mafiu\", \"Korupcia a klientelizmus je najvačši problem na Slovensku, a to hlavne vdaka SMER-SD\", \n",
    "                                 \"Zlodejstva SMERU su najvacsim problemom\"]\n",
    "\n",
    "\n",
    "utencenci_narrative = [\n",
    "    \"Utecenci su hrozba.\",\n",
    "    \"Moslimská ucelená komunita nedonesenie nič dobré.\",\n",
    "    \"Europa nedokáže zvladnuť toľko utečencov.\",\n",
    "    \"Slovensko nedokáze zvládnut tolko utečencov.\",\n",
    "    \"Pustit niekoho cez hranice bez registracie je nebezpecne.\",\n",
    "    \"My neviem, ci ti ludia nie su teroristi, s utecencami pride krimininalita a znasilnenia.\",\n",
    "    \"Kvoty na utecentov su absolutny nezmysel.\"\n",
    "]       \n",
    "\n",
    "solarne_panely = [\"solarne panely su buducnost\", \"fotovoltalika ma velky potencial\"]\n",
    "\n",
    "novinari_negativne= [\"novinari píšu len za peniaze\", \"progresivny novináry nikdy nebudú o konzervatívcoch písať pekne\", \"Novinári píšu bez akejkoľvek objektivity\",\"Novinari píšu častokrat o niečo o čom nevedia\" ]\n",
    "\n",
    "odbory = [\"odborari netvoria ziadne hodnoty, len strajkuju\", \"Odborari su komunisticky vymysel\", \"Odbori maju v nasom zakoniku prace prilis velky vplyv\"]\n",
    "\n",
    "minimalna_mzda_za = [\"Minimalna mzda je dolezity nastroj na zlepsie zivotnje urovne\", \"Zvysovanie minimalnej mzdy je dolezite pre zlepsovanie zivotnje urovne\", ]\n",
    "minimalna_mzda_proti = [\"Minimalna mzda umelo stanovuje cenu prace\", \"Minimalna mzda skodi nizkoprijmovym skupinam, lebo odrazda ostatnych abyu zamestnali\"]\n",
    "pomahanie_dochodcom = [\"Dochodcovia cely zivot pracovali a teraz by im ako stat mali pomoct\", \"Dochodcovia si zasluzia aspon nejake socialne istoty\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "narratives_dict = {\n",
    "    \"vaccine_similarity\": anti_vaccine_queries,\n",
    "    \"russian_similarity\": pro_russian_queries,\n",
    "    \"gender_similarity\": anti_gender_narrative,\n",
    "    \"ekonom_similarity\":ekonomicky_liberal,\n",
    "    \"smer_similarity\":anti_smer,\n",
    "    \"utecenci_similarity\":utencenci_narrative,\n",
    "    \"solarne_panely_similarity\":solarne_panely,\n",
    "    \"novinari_similarity\":novinari_negativne,\n",
    "    \"odbory_similarity\":odbory,\n",
    "    \"minimalna_mzda_za_similarity\":minimalna_mzda_za,\n",
    "    \"minimalna_mzda_proti_similarity\":minimalna_mzda_proti,\n",
    "    \"dochodcovia_similarity\":pomahanie_dochodcom }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multiple_similarities(\n",
    "    df,\n",
    "    embedding_col,\n",
    "    narratives_dict):\n",
    "    \"\"\"\n",
    "    For each entry in 'narratives_dict' (a dict of {similarity_col: text_list}),\n",
    "    compute a combined embedding and then calculate cosine similarities\n",
    "    against df[embedding_col].\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing existing embeddings in 'embedding_col'.\n",
    "        embedding_col (str): Column name where each row's embedding (list/array) is stored.\n",
    "        narratives_dict (dict): A dict where key is the new similarity column name,\n",
    "            and value is a list of texts that should be combined and embedded.\n",
    "        model (str): Name of the model to use for embedding.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with new similarity columns appended.\n",
    "    \"\"\"\n",
    "    for similarity_col, text_list in narratives_dict.items():\n",
    "        # 2A) Get combined embedding for the entire list of texts\n",
    "        ref_emb = get_combined_narrative_embedding(text_list)\n",
    "        # 2B) Calculate similarities for the entire DataFrame\n",
    "        df = calculate_similarity(df, embedding_col, ref_emb, similarity_col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NarativeaAnalytics(TypedDict):\n",
    "    df: pd.DataFrame\n",
    "    sample_df: pd.DataFrame\n",
    "    narratives: Annotated[list[str], add]            \n",
    "    extracted_narratives: list[str]                  \n",
    "    narratives_categories: list[str]                \n",
    "    categories_structured: list[dict] | None\n",
    "    final_analysis: list[dict] | str\n",
    "    category_stats: pd.DataFrame | None\n",
    "    topic: str\n",
    "    stance: str\n",
    "    similarity_col: str\n",
    "    embedding_col: str\n",
    "    similarity_threshold: float\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    min_iterations: int\n",
    "    candidate_count: int\n",
    "    min_candidate_rows: int\n",
    "    last_category_count: int\n",
    "    new_categories_added: int\n",
    "    new_category_slugs: list[str]           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state: NarativeaAnalytics) -> str:\n",
    "    iteration = state.get(\"iteration\", 0)\n",
    "    max_iter = state.get(\"max_iterations\", 5)\n",
    "    min_iter = state.get(\"min_iterations\", 2)\n",
    "    new_cats = state.get(\"new_categories_added\", 0)\n",
    "    # Stop if reached max\n",
    "    if iteration >= max_iter:\n",
    "        print(f\"[Router] stop: iteration {iteration} >= max {max_iter}\")\n",
    "        return \"final_controller\"\n",
    "    # Stop if after min iterations and no new categories\n",
    "    if iteration >= min_iter and new_cats == 0:\n",
    "        print(f\"[Router] stop: no new categories at iter {iteration}\")\n",
    "        return \"final_controller\"\n",
    "    return \"sample_new_speeches\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_new_speeches(state: NarativeaAnalytics):\n",
    "    df = state['df'].copy()\n",
    "    embedding_col = state['embedding_col']\n",
    "    iteration = state.get('iteration', 0)\n",
    "    similarity_col = state['similarity_col']\n",
    "    base_threshold = float(state.get('similarity_threshold', 0.5))\n",
    "    categories_structured = state.get('categories_structured') or []\n",
    "    max_per_category = 10\n",
    "    edge_low, edge_high = Edge_low, Edge_high\n",
    "\n",
    "    if 'used_for_extraction' not in df.columns:\n",
    "        df['used_for_extraction'] = False\n",
    "\n",
    "    if iteration == 0 or not categories_structured:\n",
    "        seed_narratives = state.get('narratives', []) or []\n",
    "        if not seed_narratives:\n",
    "            print(f\"[Iter {iteration}] No seed narratives available.\")\n",
    "            return {'sample_df': pd.DataFrame(), 'df': df, 'iteration': iteration + 1}\n",
    "        ref_emb = get_combined_narrative_embedding(seed_narratives)\n",
    "        df = calculate_similarity(df, embedding_col, ref_emb, similarity_col)\n",
    "        candidate_df = df[(df[similarity_col] >= base_threshold) & (~df['used_for_extraction'])]\n",
    "        if candidate_df.empty:\n",
    "            print(f\"[Iter {iteration}] centroid sampling: 0 candidates >= {base_threshold}\")\n",
    "            return {'sample_df': pd.DataFrame(), 'df': df, 'iteration': iteration + 1}\n",
    "        target = min(15, len(candidate_df))\n",
    "        group_cols = [c for c in ['obdobie','klub'] if c in candidate_df.columns]\n",
    "        if group_cols and len(candidate_df) > target:\n",
    "            work = candidate_df.copy()\n",
    "            for gc in group_cols:\n",
    "                work[gc] = work[gc].fillna('__MISSING__')\n",
    "            sizes = work.groupby(group_cols).size()\n",
    "            proportions = (sizes / sizes.sum()) * target\n",
    "            alloc = proportions.astype(int)\n",
    "            remainder = target - alloc.sum()\n",
    "            if remainder > 0:\n",
    "                frac = (proportions - alloc).sort_values(ascending=False)\n",
    "                for idx in frac.index[:remainder]:\n",
    "                    alloc.loc[idx] += 1\n",
    "            parts=[]\n",
    "            g = work.groupby(group_cols)\n",
    "            for key, need in alloc.items():\n",
    "                if need <= 0: \n",
    "                    continue\n",
    "                subset = g.get_group(key)\n",
    "                parts.append(subset.sample(min(int(need), len(subset)), random_state=42))\n",
    "            sample_df = pd.concat(parts) if parts else candidate_df.sample(target, random_state=42)\n",
    "        else:\n",
    "            sample_df = candidate_df.sample(target, random_state=42)\n",
    "        df.loc[sample_df.index,'used_for_extraction'] = True\n",
    "        print(f\"[Iter {iteration}] centroid candidates={len(candidate_df)} sampled={len(sample_df)} thr={base_threshold}\")\n",
    "        return {'sample_df': sample_df.reset_index(drop=True),\n",
    "                'df': df,\n",
    "                'iteration': iteration + 1}\n",
    "\n",
    "    # Edge-band sampling restricted ONLY to newly added categories\n",
    "    new_slugs = state.get('new_category_slugs') or []   # <--- fetch newly added category slugs\n",
    "    cat_embs = build_category_embeddings(categories_structured)\n",
    "    if new_slugs:\n",
    "        cat_embs = {k: v for k, v in cat_embs.items() if k in new_slugs}\n",
    "        if not cat_embs:\n",
    "            print(f\"[Iter {iteration}] no new categories to sample (new slugs set empty after filter)\")\n",
    "            return {'sample_df': pd.DataFrame(), 'df': df, 'iteration': iteration + 1}\n",
    "    else:\n",
    "        # No newly added categories => skip sampling (we only want new ones)\n",
    "        print(f\"[Iter {iteration}] no newly added category slugs -> skipping edge sampling\")\n",
    "        return {'sample_df': pd.DataFrame(), 'df': df, 'iteration': iteration + 1}\n",
    "\n",
    "    all_samples=[]\n",
    "    for slug, emb in cat_embs.items():\n",
    "        sim_col = f\"cat_{slug}_sim_tmp\"\n",
    "        df = calculate_similarity(df, embedding_col, emb, sim_col)\n",
    "        mask = (df[sim_col].between(edge_low, edge_high)) & (~df['used_for_extraction'])\n",
    "        cand = df[mask]\n",
    "        if cand.empty:\n",
    "            continue\n",
    "        take = min(max_per_category, len(cand))\n",
    "        picked = cand.sample(take, random_state=42).copy()\n",
    "        picked['__source_category'] = slug\n",
    "        all_samples.append(picked)\n",
    "\n",
    "    if not all_samples:\n",
    "        print(f\"[Iter {iteration}] no edge samples for new categories (band {edge_low}-{edge_high})\")\n",
    "        return {'sample_df': pd.DataFrame(), 'df': df, 'iteration': iteration + 1}\n",
    "\n",
    "    sample_df = pd.concat(all_samples)\n",
    "    sample_df = sample_df[~sample_df.index.duplicated(keep='first')]\n",
    "    df.loc[sample_df.index,'used_for_extraction'] = True\n",
    "    print(f\"[Iter {iteration}] edge sampled total={len(sample_df)} new_categories={len(cat_embs)}\")\n",
    "    return {'sample_df': sample_df.reset_index(drop=True),\n",
    "            'df': df,\n",
    "            'iteration': iteration + 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_narratives(state: NarativeaAnalytics):\n",
    "    sample_df = state[\"sample_df\"]\n",
    "    narratives = state.get(\"narratives\", [])              \n",
    "    topic = state[\"topic\"]\n",
    "    stance = state[\"stance\"]\n",
    "    iteration = state.get(\"iteration\", 0)\n",
    "\n",
    "    if sample_df is None or sample_df.empty:\n",
    "        print(f\"[Iter {iteration}] extract_narratives: empty sample\")\n",
    "        return {\"extracted_narratives\": [], \"narratives\": narratives}\n",
    "\n",
    "    text = \"\\n\\n\".join(sample_df[\"truncated_prepis\"].dropna().astype(str))\n",
    "\n",
    "    # ORIGINAL, UNCHANGED PROMPT TEXT:\n",
    "    system_msg = f\"Si asistent na detekciu naratívov k téme '{topic}'. Texty sú v slovenskom jazyku.\"\n",
    "    user_msg = (f\"\"\"Nižšie sú texty. Sú to prepisy vystupenia poslancov Národnej rady Slovenskej republiky. Tvoja úloha \n",
    "                je identifikovať a extrahovať a skopirovať relevantné výroky z textu na tému '{topic}' s týmto postojom '{stance}'. \n",
    "                Tu sú už existujúce naratívy a výroky, týmto spôsobom očakávam extraciu z textu:\\n.\n",
    "                {narratives}\n",
    "\n",
    "                Formát odpovede sú len a výlučne len skopirované výroky týkajúce sa {topic} s postojom {stance}.\\n\n",
    "              \n",
    "             \n",
    "\n",
    "                Texty:\\n\n",
    "                {text}\n",
    "                \"\"\")\n",
    "\n",
    "    content = call_openai(\n",
    "        role_message=system_msg,\n",
    "        user_message=user_msg,\n",
    "        model=model,\n",
    "        reasoning_effort=\"low\"\n",
    "    )\n",
    "\n",
    "    # Post-processing (added, prompt intact)\n",
    "    raw_items = parse_extracted_block(content)\n",
    "    # keep only NEW\n",
    "    new_items = [itm for itm in raw_items if itm not in narratives]\n",
    "    updated = narratives + new_items\n",
    "    print(f\"[Iter {iteration}] extract_narratives: new={len(new_items)} total={len(updated)}\")\n",
    "    return {\"extracted_narratives\": new_items, \"narratives\": updated}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_narratives(state: NarativeaAnalytics):\n",
    "    import json, re\n",
    "    try:\n",
    "        from pydantic import BaseModel, Field, ValidationError\n",
    "    except ImportError:\n",
    "        BaseModel = object\n",
    "        ValidationError = Exception\n",
    "        def Field(*a, **k): return None\n",
    "\n",
    "    narratives = state.get(\"narratives\", []) or []\n",
    "    existing_struct = state.get(\"categories_structured\") or []\n",
    "    prev_slugs = {_normalize_slug(c.get('name','')) for c in existing_struct}\n",
    "    topic = state[\"topic\"]; stance = state[\"stance\"]\n",
    "    iteration = state.get(\"iteration\", 0)\n",
    "\n",
    "    if not narratives:\n",
    "        print(f\"[Iter {iteration}] categorize_narratives: no narratives\")\n",
    "        return {\n",
    "            \"categories_structured\": existing_struct,\n",
    "            \"narratives_categories\": state.get(\"narratives_categories\", []),\n",
    "            \"last_category_count\": len(prev_slugs),\n",
    "            \"new_categories_added\": 0,\n",
    "            \"new_category_slugs\": []\n",
    "        }\n",
    "\n",
    "    class Category(BaseModel):\n",
    "        name: str\n",
    "        label: str\n",
    "        rationale: str\n",
    "        items: list[str]\n",
    "    class CategoryResponse(BaseModel):\n",
    "        categories: list[Category]\n",
    "\n",
    "    enumerated = \"\\n\".join(f\"{i+1}. {n}\" for i,n in enumerate(narratives))\n",
    "    system_msg = (\"Si analytický expert na zhlukovanie politických naratívov v slovenčine.\")\n",
    "    user_msg = f\"\"\"TÉMA: {topic}\n",
    "POSTOJ: {stance}\n",
    "VÝROKY:\n",
    "{enumerated}\n",
    "POŽADOVANÝ JSON:\n",
    "{{\n",
    "  \"categories\": [\n",
    "    {{\n",
    "      \"name\": \"kratky-identifikator-bez-diacritiky\",\n",
    "      \"label\": \"Čitateľný názov\",\n",
    "      \"rationale\": \"Jedna veta prečo spolu\",\n",
    "      \"items\": [\"Presný pôvodný výrok 1\",\"Presný pôvodný výrok 2\"]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "PRAVIDLÁ:\n",
    "1. Každý výrok v presne jednej kategórii.\n",
    "2. 3-15 kategórií.\n",
    "3. name lowercase bez diakritiky, hyphen separated.\n",
    "4. Iba čistý JSON.\n",
    "ODPOVEĎ:\n",
    "\"\"\"\n",
    "    raw = call_openai(system_msg, user_msg, model=model, reasoning_effort=\"low\")\n",
    "\n",
    "    def attempt(txt: str):\n",
    "        try: return json.loads(txt)\n",
    "        except:\n",
    "            m = re.search(r\"\\{[\\s\\S]*\\}\", txt)\n",
    "            if m:\n",
    "                try: return json.loads(m.group(0))\n",
    "                except: return None\n",
    "            return None\n",
    "\n",
    "    parsed = attempt(raw)\n",
    "    new_struct=[]\n",
    "    if isinstance(parsed, dict) and 'categories' in parsed:\n",
    "        try:\n",
    "            validated = CategoryResponse(**parsed)\n",
    "            new_struct = [c.model_dump() for c in validated.categories]\n",
    "        except ValidationError as ve:\n",
    "            print(f\"[Iter {iteration}] validation error: {ve.errors()[:1]}\")\n",
    "    else:\n",
    "        print(f\"[Iter {iteration}] parse failed len={len(raw)}\")\n",
    "\n",
    "    merged = {_normalize_slug(c.get('name','')): c for c in existing_struct}\n",
    "    for cat in new_struct:\n",
    "        slug = _normalize_slug(cat.get('name',''))\n",
    "        if slug in merged:\n",
    "            old_items = merged[slug].get('items', [])\n",
    "            seen=set(old_items)\n",
    "            for it in cat.get('items', []):\n",
    "                if it not in seen:\n",
    "                    old_items.append(it); seen.add(it)\n",
    "            merged[slug]['items'] = old_items\n",
    "            # keep longer rationale\n",
    "            if len(cat.get('rationale','')) > len(merged[slug].get('rationale','')):\n",
    "                merged[slug]['rationale'] = cat.get('rationale','')\n",
    "        else:\n",
    "            merged[slug] = cat\n",
    "\n",
    "    merged_struct = list(merged.values())\n",
    "    new_slugs = [slug for slug in merged if slug not in prev_slugs]   # <--- identify new category slugs\n",
    "    new_count = len(new_slugs)\n",
    "\n",
    "    flat=[]\n",
    "    for c in merged_struct:\n",
    "        slug=_normalize_slug(c.get('name',''))\n",
    "        for it in c.get('items', []):\n",
    "            flat.append(f\"{slug} | {it}\")\n",
    "    # dedup\n",
    "    dedup=[]; seen=set()\n",
    "    for line in flat:\n",
    "        if line not in seen:\n",
    "            seen.add(line); dedup.append(line)\n",
    "\n",
    "    print(f\"[Iter {iteration}] categorize_narratives: prev={len(prev_slugs)} now={len(merged)} new={new_count} narratives={len(narratives)}\")\n",
    "    if new_slugs:\n",
    "        print(f\"[Iter {iteration}] new category slugs: {new_slugs}\")\n",
    "    return {\n",
    "        \"categories_structured\": merged_struct,\n",
    "        \"narratives_categories\": dedup,\n",
    "        \"last_category_count\": len(merged),\n",
    "        \"new_categories_added\": new_count,\n",
    "        \"new_category_slugs\": new_slugs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_controller(state: NarativeaAnalytics):\n",
    "    categories = state.get(\"categories_structured\", [])\n",
    "    topic = state[\"topic\"]\n",
    "    stance = state[\"stance\"]\n",
    "    iteration = state.get(\"iteration\", 0)\n",
    "\n",
    "    if not categories:\n",
    "        print(f\"[Final Controller] No categories to process\")\n",
    "        return {\"final_analysis\": \"No categories found for final analysis.\"}\n",
    "\n",
    "    system_msg = (\"Si analytický expert na sumarizáciu politických naratívov v slovenčine\")\n",
    "    user_msg = f\"\"\" K dispozícii máš text s výrokmi poslancov NRSR na tému '{topic}' s týmto postojom '{stance}'. \n",
    "Tieto výroky pochádzajú z prepisov vystupení poslancov v Národnej rady Slovenskej republiky. Výroky týkajuce sa {topic} a {stance} boli hladané v prepisoch a skopirované to materiálu. \n",
    "Neskôr boli jednotlivé výroky kategorizované.\n",
    "\n",
    "Inštrukcie:\n",
    "1. Celý materál si prečitaš\n",
    "2. Výroky sú doslovné citácie, a niekedy sa dostali do textu aj prepisom, ktorý už nesúvisí s témou '{topic}' s týmto postojom '{stance}'. \n",
    "   Teda ten, kto to prepisoval spravil chybu a namiesto len výroku prepisal celé vystúpenie, alebo jeho časť. Tvoja úloha je identifikovať a ponechať len výrok.\n",
    "3. Ak je kategórií viac ako 10, tak vytvoríš nové podkategórie. Semanticky zgrupuješ podobné kategórie do jedného celku aj s ich výrokmi\n",
    "4. Štruktúra odpovede je taká istá ako na vstupe. Len je tam menej kategórií nakoľko si ich zgrupoval aj s výrokmi. Výroky ponecháš v pôvodnom znení, ktoré si krátil o nesúvisiaci text.\n",
    "5. Formát JSON s polami: {{\"categories\": [{{\"name\": \"slug\", \"label\": \"názov\", \"rationale\": \"zdôvodnenie\", \"items\": [\"výrok1\", \"výrok2\"]}}]}}\n",
    "\n",
    "VSTUPNÉ KATEGÓRIE:\n",
    "{categories}\n",
    "\"\"\"\n",
    "\n",
    "    content = call_openai(system_msg, user_msg, model=\"gpt-5\", reasoning_effort=\"high\")\n",
    "\n",
    "    # Parse the LLM response to extract structured categories\n",
    "    import json, re\n",
    "    try:\n",
    "        from pydantic import BaseModel, Field, ValidationError\n",
    "    except ImportError:\n",
    "        BaseModel = object\n",
    "        ValidationError = Exception\n",
    "\n",
    "    class Category(BaseModel):\n",
    "        name: str\n",
    "        label: str  \n",
    "        rationale: str\n",
    "        items: list[str]\n",
    "    class CategoryResponse(BaseModel):\n",
    "        categories: list[Category]\n",
    "\n",
    "    def attempt_parse(txt: str):\n",
    "        try: \n",
    "            return json.loads(txt)\n",
    "        except:\n",
    "            m = re.search(r\"\\{[\\s\\S]*\\}\", txt)\n",
    "            if m:\n",
    "                try: \n",
    "                    return json.loads(m.group(0))\n",
    "                except: \n",
    "                    return None\n",
    "            return None\n",
    "\n",
    "    parsed = attempt_parse(content)\n",
    "    \n",
    "    if isinstance(parsed, dict) and 'categories' in parsed:\n",
    "        try:\n",
    "            validated = CategoryResponse(**parsed)\n",
    "            final_struct = [c.model_dump() for c in validated.categories]\n",
    "            print(f\"[Final Controller] Successfully processed {len(categories)} -> {len(final_struct)} final categories\")\n",
    "            \n",
    "            # Return ONLY the final analysis as structured data\n",
    "            return {\"final_analysis\": final_struct}\n",
    "            \n",
    "        except ValidationError as ve:\n",
    "            print(f\"[Final Controller] Validation error: {ve.errors()[:1]}\")\n",
    "            # Fallback: return original structure  \n",
    "            return {\"final_analysis\": categories}\n",
    "    else:\n",
    "        print(f\"[Final Controller] Parse failed, returning original structure\")\n",
    "        return {\"final_analysis\": categories}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_to_docx(narratives, filename):\n",
    "    doc = Document()\n",
    "    doc.add_heading('Rozsirene_narrativy', level=1)\n",
    "    \n",
    "    for narrative in narratives:\n",
    "        if narrative is not None:\n",
    "            doc.add_paragraph(str(narrative))\n",
    "    \n",
    "    doc.save(filename)\n",
    "    print(f\"Document saved as {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rebuild graph without analyze_categories\n",
    "graph = StateGraph(NarativeaAnalytics)\n",
    "graph.add_node(\"sample_new_speeches\", sample_new_speeches)\n",
    "graph.add_node(\"extract_narratives\", extract_narratives)\n",
    "graph.add_node(\"categorize_narratives\", categorize_narratives)\n",
    "graph.add_node(\"final_controller\", final_controller)\n",
    "\n",
    "graph.add_edge(START, \"sample_new_speeches\")\n",
    "graph.add_edge(\"sample_new_speeches\", \"extract_narratives\")\n",
    "graph.add_edge(\"extract_narratives\", \"categorize_narratives\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"categorize_narratives\",\n",
    "    router,\n",
    "    {\n",
    "        \"sample_new_speeches\": \"sample_new_speeches\",\n",
    "        \"final_controller\": \"final_controller\"\n",
    "    }\n",
    ") \n",
    "\n",
    "\n",
    "graph.add_edge(\"final_controller\", END)\n",
    "graph = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_pipeline(df,\n",
    "                   seed_narratives,\n",
    "                   topic,\n",
    "                   stance,\n",
    "                   similarity_col,\n",
    "                   similarity_threshold: float = 0.50,\n",
    "                   min_iterations: int = 2,\n",
    "                   max_iterations: int = 3,\n",
    "                   embedding_col: str | None = None,\n",
    "                   doc_export: bool = True):\n",
    "    if embedding_col is None:\n",
    "        for cand in [\"openAI_embedding_small\",\"openAI_embedding_3076\",\"voyage-3-large_embeddings\",\"mistral_embedings\"]:\n",
    "            if cand in df.columns:\n",
    "                embedding_col = cand; break\n",
    "    if embedding_col is None:\n",
    "        raise ValueError(\"Embedding column not found.\")\n",
    "\n",
    "    if similarity_col not in df.columns:\n",
    "        df[similarity_col] = 0.0\n",
    "\n",
    "    init_state: NarativeaAnalytics = NarativeaAnalytics(\n",
    "        df=df,\n",
    "        sample_df=pd.DataFrame(),\n",
    "        narratives=seed_narratives,\n",
    "        extracted_narratives=[],\n",
    "        narratives_categories=[],\n",
    "        categories_structured=[],\n",
    "        final_analysis=\"\",\n",
    "        category_stats=None,\n",
    "        topic=topic,\n",
    "        stance=stance,\n",
    "        similarity_col=similarity_col,\n",
    "        embedding_col=embedding_col,\n",
    "        similarity_threshold=similarity_threshold,\n",
    "        iteration=0,\n",
    "        max_iterations=max_iterations,\n",
    "        min_iterations=min_iterations,\n",
    "        candidate_count=0,\n",
    "        min_candidate_rows=0,\n",
    "        last_category_count=0,\n",
    "        new_categories_added=0\n",
    "    )\n",
    "\n",
    "    state = graph.invoke(init_state)\n",
    "\n",
    "    if doc_export:\n",
    "        save_to_docx(state.get(\"narratives\", []), rf\"..\\data\\politico_agent_texts\\{topic}_narratives_{model}.docx\")\n",
    "        save_to_docx(state.get(\"narratives_categories\", []), rf\"..\\data\\politico_agent_texts\\{topic}_narratives_categories_{model}.docx\")\n",
    "        if state.get(\"category_stats\") is not None and not state[\"category_stats\"].empty:\n",
    "            state[\"category_stats\"].to_excel(rf\"..\\data\\politico_agent_texts\\category_stats_{topic}.xlsx\", index=False)\n",
    "\n",
    "    print(f\"Finished iterations={state.get('iteration')} categories={state.get('last_category_count')} narratives={len(state.get('narratives',[]))}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 0] centroid candidates=177 sampled=15 thr=0.5\n",
      "[Iter 1] extract_narratives: new=14 total=19\n",
      "[Iter 1] categorize_narratives: prev=0 now=8 new=8 narratives=24\n",
      "[Iter 1] new category slugs: ['rusko-neohrozuje', 'nato-rozsirovanie-ukrajina', 'proti-dodavkam-zbrani-a-eskalacii', 'whataboutizmus-usa-nato', 'sankcie-skodia-slovensku', 'rusko-neporazitelne-jadrova-mocnost', 'zbrojne-rozpocty-porovnanie', 'dobre-vztahy-s-ruskom']\n",
      "[Iter 1] edge sampled total=73 new_categories=8\n",
      "[Iter 2] extract_narratives: new=19 total=43\n",
      "[Iter 2] categorize_narratives: prev=8 now=20 new=12 narratives=67\n",
      "[Iter 2] new category slugs: ['rusko-nas-nepriatel-nie', 'nato-ukrajina-rozsirovanie', 'proti-dodavkam-zbrani-za-mier', 'whataboutizmus-usa', 'sankcie-a-energie-skodia-nam', 'rusko-neporazitelne-a-uzemia-udrzi', 'rusko-neni-hrozba', 'slovensko-samoviina-za-zhorsenie-vztahov', 'kritika-nato-a-zapadu', 'krym-je-opravneny-pripad', 'vdaka-sssr-a-rusom-za-oslobodenie', 'obhajoba-krokov-sssr']\n",
      "[Iter 2] edge sampled total=115 new_categories=12\n",
      "[Iter 3] extract_narratives: new=18 total=85\n",
      "[Iter 3] categorize_narratives: prev=20 now=30 new=10 narratives=152\n",
      "[Iter 3] new category slugs: ['rusko-nie-je-nepriatel-dobre-vztahy', 'sankcie-a-energia-proti-nam', 'proti-dodavkam-zbrani-predlzuju-konflikt', 'neda-sa-porazit-rusko-ani-nie-je-hrozbou', 'nato-provokacie-rozsirovanie-a-odmietnutie', 'krym-a-sebaurcenie', 'whataboutizmus-irak-juhoslavia-kosovo', 'historicke-obhajoby-zssr-a-slovanska-jednota', 'rusko-neohrozi-slovensko', 'propaganda-zapadu-a-proxy-vojna']\n",
      "[Router] stop: iteration 3 >= max 3\n",
      "[Final Controller] Successfully processed 30 -> 10 final categories\n",
      "Document saved as ..\\data\\politico_agent_texts\\Ruska zahranicna politika, vztahy s Ruskom, Ruske ekonomicke zaujmu_narratives_gpt-5.docx\n",
      "Document saved as ..\\data\\politico_agent_texts\\Ruska zahranicna politika, vztahy s Ruskom, Ruske ekonomicke zaujmu_narratives_categories_gpt-5.docx\n",
      "Finished iterations=3 categories=30 narratives=152\n"
     ]
    }
   ],
   "source": [
    "state_russian = agent_pipeline(df, pro_russian_queries, \"Ruska zahranicna politika, vztahy s Ruskom, Ruske ekonomicke zaujmu\", \"pro-Rusky, podporujuci rusko\", similarity_col =\"russian_similarity\", embedding_col=\"openAI_embedding_3076\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = state_russian[\"final_analysis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document saved as ..\\data\\politico_agent_texts\\final_analysis_gpt-5_russian.docx\n"
     ]
    }
   ],
   "source": [
    "save_to_docx(state_russian[\"final_analysis\"], rf\"..\\data\\politico_agent_texts\\final_analysis_{model}_russian.docx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "....................................................................................................................................................................................."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
